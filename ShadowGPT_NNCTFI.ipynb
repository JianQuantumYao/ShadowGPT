{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b777686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, head_count):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, head_count, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(4*embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attention_output, _ = self.attention(x_norm,x_norm,x_norm)\n",
    "        x = x + self.dropout(attention_output)\n",
    "        x_norm = self.norm2(x)\n",
    "        ff_output = self.feed_forward(x_norm)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        return x\n",
    "    \n",
    "class Transformer_learnable(nn.Module):\n",
    "    def __init__(self, vocab_size, posi_size, embed_size, num_layers, head_count):\n",
    "        super(Transformer_learnable, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(posi_size, embed_size)\n",
    "        self.para_embedding = nn.Sequential(\n",
    "#             nn.LayerNorm(3),\n",
    "            nn.Linear(3, 2*embed_size),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.LayerNorm(2*embed_size),\n",
    "            nn.Linear(2*embed_size, embed_size * 2),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.LayerNorm(embed_size*2),\n",
    "            nn.Linear(embed_size * 2, embed_size)\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(embed_size, head_count) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, 2)\n",
    "        self.tokendrop = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, inputs, device, mask=None):\n",
    "        para_inputs = inputs[:, :3].view(-1, 3).to(device).float()\n",
    "        para_inputs = para_inputs / torch.sum(para_inputs, dim=1, keepdim=True)\n",
    "        para_context = self.para_embedding(para_inputs).unsqueeze(1)  # (batch_size, 1, embed_size)      \n",
    "        # Token embeddings\n",
    "        input_tokens = inputs[:, 3:].to(torch.long).to(device)\n",
    "        batch_size, token_count = input_tokens.shape[:2]\n",
    "        token_embeddings = self.word_embedding(input_tokens)      \n",
    "        # Sinusoidal positional encodings\n",
    "        positions = torch.arange(0, token_count).expand(batch_size, token_count).to(device)\n",
    "        position_embeddings = self.position_embedding(positions)    \n",
    "        out = token_embeddings+position_embeddings     \n",
    "        out = torch.cat((para_context, out), dim=1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)       \n",
    "        # Final output prediction\n",
    "        out = self.fc_out(out[:, -1, :].reshape(batch_size, self.embed_size)).reshape(batch_size, 2)\n",
    "        return out\n",
    "        \n",
    "def generate_GPTmeasureoutput(J1, J2, J3, model, num_qubits, sample_size, device):\n",
    "    # Preallocate the outputs tensor to avoid repeated concatenation\n",
    "    outputs = torch.full((sample_size, 3 + num_qubits * 2), 0.0, device=device)# \"0.0\" to set J1 J2 J3 float\n",
    "    outputs[:, :3] = torch.tensor([J1, J2, J3], device=device).expand(sample_size, 3)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_qubits):\n",
    "            # Assign random input in the corresponding position\n",
    "            random_input = torch.randint(2, 5, (sample_size,), device=device, dtype=torch.long)\n",
    "            outputs[:, 3 + 2 * i] = random_input\n",
    "            # Forward pass through the model\n",
    "            measurepro = model(outputs[:, :(3 + 2 * i + 1)], device)\n",
    "            measurepro = torch.nn.functional.softmax(measurepro, dim=1)\n",
    "            # Generate new index based on softmax probabilities\n",
    "            newindex = torch.bernoulli(measurepro[:, 1].clamp(min=0, max=1)).to(torch.long)\n",
    "            outputs[:, 4 + 2 * i] = newindex\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,measureResults):\n",
    "        super().__init__()\n",
    "        self.results = measureResults\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        squence = self.results[index]\n",
    "        return squence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.results)\n",
    "    \n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "train_dataset = torch.load('symmodified_NNCTFI_pbc_dataset_4to10bit_10000samples_each.pt')[6*240000:].to(device)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_sampleResults = torch.load('symmodified_NNCTFI_pbc_dataset_15and9_every500_10bit.pt')\n",
    "test_dataset = myDataset(measureResults=test_sampleResults)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(train_dataset.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn, optim\n",
    "writer = SummaryWriter()\n",
    "def train_recursive(model, dataloader, optimizer, loss_fn, epoch, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0  # Initialize total loss\n",
    "    num_batches = len(dataloader)\n",
    "    for batch_idx, dataset in enumerate(dataloader):\n",
    "        dataset = dataset.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_size, token_count = dataset.shape[0], dataset.shape[1]\n",
    "        loss = 0\n",
    "        for cur_count in range(4, token_count):# different from \"(2,token_count)\" in training of TFI model\n",
    "            if cur_count % 2 == 0:\n",
    "                expected_next_token_indexes = dataset[:, cur_count] \n",
    "                model_input = dataset[:, :cur_count] \n",
    "                outputs = model(model_input, device) \n",
    "                loss += loss_fn(outputs, expected_next_token_indexes.to(torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1} Train Loss: {average_loss:.6f}\")\n",
    "    writer.add_scalar(\"Loss/train\", average_loss, epoch + 1)\n",
    "\n",
    "\n",
    "def test_recursive(model, dataloader, loss_fn, epoch, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dataset in dataloader:\n",
    "            dataset = dataset.to(device)\n",
    "            batch_size, token_count = dataset.shape[0], dataset.shape[1]\n",
    "            loss = 0\n",
    "            for cur_count in range(4, token_count):\n",
    "                if cur_count % 2 == 0:\n",
    "                    expected_next_token_indexes = dataset[:, cur_count]\n",
    "                    model_input = dataset[:, :cur_count]\n",
    "                    outputs = model(model_input, device)\n",
    "                    loss += loss_fn(outputs, expected_next_token_indexes.to(torch.long))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1} Test Loss: {average_loss:.6f}\")\n",
    "    writer.add_scalar(\"Loss/test\", average_loss, epoch + 1)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "vocab_size = 5 \n",
    "posi_size = 20 \n",
    "embed_size = 128\n",
    "num_layers = 4\n",
    "head_count = 4\n",
    "\n",
    "model = Transformer_learnable(vocab_size, posi_size, embed_size, num_layers, head_count)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 80\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\\n{'-'*30}\")\n",
    "    train_recursive(model, train_loader, optimizer, loss_fn, epoch, device)\n",
    "    current_test_loss = test_recursive(model, test_loader, loss_fn, epoch, device)\n",
    "    # Save the best model based on test loss\n",
    "    if current_test_loss < best_test_loss:\n",
    "        best_test_loss = current_test_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_fn\n",
    "        }, 'GPT_symmodified_NNCTFI_prompt_pbc_dataset_15and9_10000_10bit_801e-4_pn_jump_nodropoutlayernorm_large_best.pth')\n",
    "        print(\"Saved Best Model\")\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "# Save the final model\n",
    "torch.save({\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss_fn\n",
    "}, 'GPT_symmodified_NNCTFI_prompt_pbc_dataset_15and9_10000_10bit_801e-4_pn_jump_nodropoutlayernorm_large.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from qiskit.quantum_info import partial_trace, DensityMatrix\n",
    "\n",
    "X = torch.tensor([[0.,1.],[1.,0.]], dtype = torch.cfloat)\n",
    "Y = torch.tensor([[0.,-1.j],[1.j,0.]], dtype = torch.cfloat)\n",
    "Z = torch.tensor([[1.,0.],[0.,-1.]], dtype = torch.cfloat)\n",
    "I = torch.tensor([[1.,0.],[0.,1.]], dtype = torch.cfloat)\n",
    "operator_map = {0:I, 1:X, 2:Y, 3:Z}\n",
    "\n",
    "def correlation_func(sqe_op):\n",
    "    func = operator_map[int(sqe_op[0])]\n",
    "    for i in range(len(sqe_op)-1):\n",
    "        func = torch.kron(func,operator_map[int(sqe_op[i+1])])\n",
    "    return func\n",
    "\n",
    "def symmetry_proj(rho, num_qubits):\n",
    "    smop1_index = np.zeros(num_qubits)\n",
    "    smop1 = correlation_func(smop1_index)\n",
    "    smop2_index = np.zeros(num_qubits)\n",
    "    smop2_index[::2] = 1\n",
    "    smop2 = correlation_func(smop2_index)\n",
    "    smop3_index = np.zeros(num_qubits)\n",
    "    smop3_index[1::2] = 1\n",
    "    smop3 = correlation_func(smop3_index)\n",
    "    smop4_index = np.ones(num_qubits)\n",
    "    smop4 = correlation_func(smop4_index)\n",
    "    rho_ = rho+smop2@rho@smop2.H+smop3@rho@smop3.H+smop4@rho@smop4.H\n",
    "    return rho_/np.trace(rho_)\n",
    "\n",
    "\n",
    "# Hamiltonian of NNCTFI (or ZZ2_X_ZXZ), notice that in the paper, the coefficients are g1, g2, g3\n",
    "def Ham_ZZ2_X_ZXZ(J1,J2,J3,num_qubits):\n",
    "    ham = 0.+0.j\n",
    "    for i in range(num_qubits-2):\n",
    "        ZZ_index = np.zeros(num_qubits)\n",
    "        ZZ_index[i], ZZ_index[i+2] = 3, 3\n",
    "        ham -= J1*correlation_func(ZZ_index)\n",
    "    ZZ_index = np.zeros(num_qubits)\n",
    "    ZZ_index[-2], ZZ_index[0] = 3, 3\n",
    "    ham -= J1*correlation_func(ZZ_index)\n",
    "    \n",
    "    ZZ_index = np.zeros(num_qubits)\n",
    "    ZZ_index[-1], ZZ_index[1] = 3, 3\n",
    "    ham -= J1*correlation_func(ZZ_index)\n",
    "    \n",
    "    for i in range(num_qubits):\n",
    "        X_index = np.zeros(num_qubits)\n",
    "        X_index[i] = 1\n",
    "        ham -= J2*correlation_func(X_index)\n",
    "        \n",
    "    for i in range(1, num_qubits-1):\n",
    "        ZXZ_index = np.zeros(num_qubits)\n",
    "        ZXZ_index[i-1], ZXZ_index[i], ZXZ_index[i+1] = 3, 1, 3\n",
    "        ham -= J3*correlation_func(ZXZ_index)\n",
    "    \n",
    "    ZXZ_index = np.zeros(num_qubits)\n",
    "    ZXZ_index[-2], ZXZ_index[-1], ZXZ_index[0] = 3, 1, 3\n",
    "    ham -= J3*correlation_func(ZXZ_index)\n",
    "    \n",
    "    ZXZ_index = np.zeros(num_qubits)\n",
    "    ZXZ_index[-1], ZXZ_index[0], ZXZ_index[1] = 3, 1, 3\n",
    "    ham -= J3*correlation_func(ZXZ_index)\n",
    "    \n",
    "    return ham\n",
    "\n",
    "def truth_cal_averge_two_point_correlation(J1,J2,J3,num_bits,d):\n",
    "    Ham = Ham_ZZ2_X_ZXZ(J1,J2,J3,num_bits)\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(Ham)\n",
    "    ground = eigenvectors[:,0].view(-1,1)\n",
    "    rho = ground@ground.H\n",
    "    nmz = 0\n",
    "    for i in range(num_bits):\n",
    "        ZZ_index = np.zeros(num_bits)\n",
    "        ZZ_index[i] = 3\n",
    "        ZZ_index[(i+d)%num_bits] = 3\n",
    "        ope = correlation_func(ZZ_index)\n",
    "        nmz += torch.trace(rho@ope)\n",
    "    return nmz.real/(num_bits)\n",
    "\n",
    "def sqe_cal_averge_two_point_pbc(sqe, d, device):\n",
    "    sqe = sqe.to(device)\n",
    "    num_bits = int(sqe.shape[1]/2)\n",
    "    nmz = 0\n",
    "    expZ = torch.zeros(sqe.shape[0], sqe.shape[1] // 2, dtype=sqe.dtype).to(device)\n",
    "    cond_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 1)\n",
    "    cond_neg_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 0)\n",
    "    for i in range(0, expZ.shape[1] * 2, 2):\n",
    "        expZ[:, i // 2][cond_3_Z[:, i]] = 3\n",
    "        expZ[:, i // 2][cond_neg_3_Z[:, i]] = -3\n",
    "    expZZ = expZ* torch.cat((expZ[:,d:],expZ[:,:d]),dim=1)\n",
    "    return torch.sum(expZZ)/(num_bits*sqe.shape[0])\n",
    "\n",
    "def truth_cal_groundenergy(J1,J2,J3,num_bits):\n",
    "    Ham = Ham_ZZ2_X_ZXZ(J1,J2,J3,num_bits)\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(Ham)\n",
    "    return eigenvalues[0].item()\n",
    "\n",
    "def sqe_cal_groundenergy(sqe,J1,J2,J3,device):\n",
    "    sqe = sqe.to(device)\n",
    "    num_bits = int(sqe.shape[1]/2)\n",
    "    T = sqe.shape[0]\n",
    "    expX = torch.zeros(sqe.shape[0], sqe.shape[1] // 2, dtype=sqe.dtype).to(device)\n",
    "    cond_3_X = (sqe[:, :-1] == 2) & (sqe[:, 1:] == 1)\n",
    "    cond_neg_3_X = (sqe[:, :-1] == 2) & (sqe[:, 1:] == 0)\n",
    "    for i in range(0, expX.shape[1] * 2, 2):\n",
    "        expX[:, i // 2][cond_3_X[:, i]] = 3\n",
    "        expX[:, i // 2][cond_neg_3_X[:, i]] = -3\n",
    "        \n",
    "    expZ = torch.zeros(sqe.shape[0], sqe.shape[1] // 2, dtype=sqe.dtype).to(device)\n",
    "    cond_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 1)\n",
    "    cond_neg_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 0)\n",
    "    for i in range(0, expZ.shape[1] * 2, 2):\n",
    "        expZ[:, i // 2][cond_3_Z[:, i]] = 3\n",
    "        expZ[:, i // 2][cond_neg_3_Z[:, i]] = -3\n",
    "    \n",
    "    expZZ = expZ[:,:-2]*expZ[:,2:]\n",
    "    expZXZ = expZ[:,:-2]*expX[:,1:-1]*expZ[:,2:]\n",
    "#     periodic items\n",
    "    ZZ1 = expZ[:,-2]*expZ[:,0]\n",
    "    ZZ2 = expZ[:,-1]*expZ[:,1]\n",
    "    \n",
    "    ZXZ1 = expZ[:,-2]*expX[:,-1]*expZ[:,0]\n",
    "    ZXZ2 = expZ[:,-1]*expX[:,0]*expZ[:,1]\n",
    "    \n",
    "    return -(J1*(torch.sum(expZZ)+torch.sum(ZZ1)+torch.sum(ZZ2))+J2*torch.sum(expX)+J3*(torch.sum(expZXZ)+torch.sum(ZXZ1)+torch.sum(ZXZ2)))/T\n",
    "\n",
    "# true value of Renyi entropy\n",
    "def truth_cal_renyi(J1,J2,J3,num_bits,res_num_bits):\n",
    "    Ham = Ham_ZZ2_X_ZXZ(J1,J2,J3,num_bits)\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(Ham)\n",
    "    ground = eigenvectors[:,0].view(-1,1)\n",
    "    rho = ground@ground.H\n",
    "    rho = symmetry_proj(rho,num_bits)\n",
    "    rho_den = DensityMatrix(rho.numpy())\n",
    "    rhoL = partial_trace(rho_den,list(range(int(num_bits-res_num_bits))))\n",
    "    purity_value = rhoL.purity().real\n",
    "    purity_array = np.array([purity_value])\n",
    "    return -torch.log2(torch.from_numpy(purity_array))\n",
    "\n",
    "# calculate Renyi entropy given the sequences generated by the transformer, only the part of left qubits is required\n",
    "mapping = {(2, 0): 0,(2, 1): 1,(3, 0): 2,(3, 1): 3,(4, 0): 4,(4, 1): 5}\n",
    "precomputed_f_matrix = torch.tensor([[ 5.0000, -4.0000,  0.5000,  0.5000,  0.5000,  0.5000],\n",
    "        [-4.0000,  5.0000,  0.5000,  0.5000,  0.5000,  0.5000],\n",
    "        [ 0.5000,  0.5000,  5.0000, -4.0000,  0.5000,  0.5000],\n",
    "        [ 0.5000,  0.5000, -4.0000,  5.0000,  0.5000,  0.5000],\n",
    "        [ 0.5000,  0.5000,  0.5000,  0.5000,  5.0000, -4.0000],\n",
    "        [ 0.5000,  0.5000,  0.5000,  0.5000, -4.0000,  5.0000]])\n",
    "def obtain_mappedseq(sequences, device):\n",
    "    T, length = sequences.shape\n",
    "    num_bits = length // 2\n",
    "    pairs = sequences.view(T, num_bits, 2).to(device)\n",
    "    mapping_tensor = torch.full((5, 6), -1, dtype=torch.int64, device=device)\n",
    "    for (key, value) in mapping.items():\n",
    "        mapping_tensor[key] = value\n",
    "    mapped_sequences = mapping_tensor[pairs[..., 0], pairs[..., 1]]\n",
    "    return mapped_sequences\n",
    "def seq_cal_renyi(seq, device):\n",
    "    device = torch.device(device)\n",
    "    seq = seq.to(device)\n",
    "    mapped_sequences = obtain_mappedseq(seq, device)  \n",
    "    f_values = precomputed_f_matrix.to(device)[mapped_sequences.unsqueeze(1), mapped_sequences.unsqueeze(0)]\n",
    "    product_f = f_values.prod(dim=-1)\n",
    "    T = seq.shape[0] \n",
    "    mask = (torch.ones((T, T), dtype=torch.float32, device=device) - torch.eye(T, device=device))\n",
    "    sum_f = (product_f * mask).sum()\n",
    "    result = sum_f / (T * (T - 1))\n",
    "    \n",
    "    return -torch.log2(result)\n",
    "\n",
    "# Median of means calculation, similar to functions for TFI model\n",
    "def median_of_means_renyi(sqe, device, num_parts):\n",
    "    T, _ = sqe.shape\n",
    "    part_size = T // num_parts\n",
    "    \n",
    "    means = []\n",
    "    for i in range(num_parts):\n",
    "        part_sqe = sqe[i*part_size : (i+1)*part_size]\n",
    "        mean_value = seq_cal_renyi(part_sqe,device)\n",
    "        means.append(mean_value.item())\n",
    "    \n",
    "    median_value = torch.median(torch.tensor(means))\n",
    "    return median_value\n",
    "\n",
    "def MoM_energy(J1,J2,J3,sqe,device,num_parts):\n",
    "    T, _ = sqe.shape\n",
    "    part_size = T // num_parts\n",
    "    \n",
    "    means = []\n",
    "    for i in range(num_parts):\n",
    "        part_sqe = sqe[i*part_size : (i+1)*part_size]\n",
    "        mean_value = sqe_cal_groundenergy(part_sqe,J1,J2,J3,device)\n",
    "        means.append(mean_value.item())\n",
    "    \n",
    "    median_value = torch.median(torch.tensor(means))\n",
    "    return median_value\n",
    "\n",
    "def MoM_twopoint(sqe,d,device,num_parts):\n",
    "    T, _ = sqe.shape\n",
    "    part_size = T // num_parts\n",
    "    \n",
    "    means = []\n",
    "    for i in range(num_parts):\n",
    "        part_sqe = sqe[i*part_size : (i+1)*part_size]\n",
    "        mean_value = sqe_cal_averge_two_point_pbc(part_sqe,d,device)\n",
    "        means.append(mean_value.item())\n",
    "    \n",
    "    median_value = torch.median(torch.tensor(means))\n",
    "    return median_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991afb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of prediction\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "num_bits = 10\n",
    "vocab_size = 5\n",
    "embed_size = 128\n",
    "posi_size = 20\n",
    "num_layers = 4\n",
    "head_count = 4\n",
    "model = Transformer_learnable(vocab_size, posi_size, embed_size, num_layers, head_count)\n",
    "checkpoint = torch.load('GPT_symmodified_NNCTFI_prompt_pbc_dataset_15and9_10000_10bit_801e-4_pn_jump_nodropoutlayernorm_large_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_uniform_simplex_samples(n_points):\n",
    "    samples = []\n",
    "    for i in range(n_points + 1):\n",
    "        for j in range(n_points - i + 1):\n",
    "            J1 = i / n_points\n",
    "            J2 = j / n_points\n",
    "            J3 = 1 - J1 - J2\n",
    "            samples.append([J1, J2, J3])\n",
    "    return np.array(samples)\n",
    "\n",
    "# Number of samples along one axis\n",
    "n_points = 20\n",
    "# Generate uniform samples in the simplex\n",
    "simplex_samples = generate_uniform_simplex_samples(n_points)\n",
    "energy_list = []\n",
    "twopoint_list = []\n",
    "renyi_5_list = []\n",
    "renyi_4_list = []\n",
    "renyi_3_list = []\n",
    "\n",
    "for sample in simplex_samples:\n",
    "    J1, J2, J3 = torch.from_numpy(sample).to(torch.float)\n",
    "\n",
    "    # Generate sequences and calculate energy and two-point correlation\n",
    "    seq = generate_GPTmeasureoutput(J1, J2, J3, model, 10, 200000, device)[:, 3:].to(torch.long)\n",
    "    energy = MoM_energy(J1, J2, J3, seq, device, 10)\n",
    "    twopoint = MoM_twopoint(seq, 2, device, 10)\n",
    "    energy_list.append(energy)\n",
    "    twopoint_list.append(twopoint)\n",
    "    torch.cuda.empty_cache()\n",
    "    # Generate sequences and calculate Renyi entropy\n",
    "    seq_renyi = generate_GPTmeasureoutput(J1, J2, J3, model, 5, 300000, device)[:, 3:].to(torch.long)\n",
    "    renyi_5 = median_of_means_renyi(seq_renyi, device, 10)\n",
    "    renyi_4 = median_of_means_renyi(seq_renyi[:, :-2], device, 10)\n",
    "    renyi_3 = median_of_means_renyi(seq_renyi[:, :-4], device, 10)\n",
    "    renyi_5_list.append(renyi_5)\n",
    "    renyi_4_list.append(renyi_4)\n",
    "    renyi_3_list.append(renyi_3)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "energy_list = torch.tensor(energy_list)\n",
    "twopoint_list = torch.tensor(twopoint_list)\n",
    "renyi_5_list = torch.tensor(renyi_5_list)\n",
    "renyi_4_list = torch.tensor(renyi_4_list)\n",
    "renyi_3_list = torch.tensor(renyi_3_list)\n",
    "\n",
    "# Save results\n",
    "torch.save(energy_list, 'NNCTFI_energy_list.pt')\n",
    "torch.save(twopoint_list, 'NNCTFI_twopoint_list.pt')\n",
    "torch.save(renyi_5_list, 'NNCTFI_renyi_5_list.pt')\n",
    "torch.save(renyi_4_list, 'NNCTFI_renyi_4_list.pt')\n",
    "torch.save(renyi_3_list, 'NNCTFI_renyi_3_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code of predictio when you want to show results on a 3D image\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "num_bits=10\n",
    "\n",
    "vocab_size = 5 \n",
    "embed_size = 128\n",
    "posi_size = 20\n",
    "num_layers = 4\n",
    "head_count = 4\n",
    "model = Transformer_learnable(vocab_size, posi_size, embed_size, num_layers, head_count)\n",
    "checkpoint = torch.load('GPT_symmodified_NNCTFI_prompt_pbc_dataset_15and9_10000_10bit_801e-4_pn_jump_nodropoutlayernorm_large_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_uniform_simplex_samples(n_points):\n",
    "    samples = []\n",
    "    for i in range(n_points + 1):\n",
    "        for j in range(n_points - i + 1):\n",
    "            J1 = i / n_points\n",
    "            J2 = j / n_points\n",
    "            J3 = 1 - J1 - J2\n",
    "            samples.append([J1, J2, J3])\n",
    "    return np.array(samples)\n",
    "\n",
    "# Number of samples along one axis\n",
    "n_points = 20\n",
    "\n",
    "# Generate uniform samples in the simplex\n",
    "simplex_samples = generate_uniform_simplex_samples(n_points)\n",
    "\n",
    "# Prepare lists to hold the values\n",
    "J1_list = []\n",
    "J2_list = []\n",
    "J3_list = []\n",
    "twopoint_list = []\n",
    "\n",
    "# Compute energy for each sample in the simplex\n",
    "for sample in simplex_samples:\n",
    "    J1, J2, J3 = torch.from_numpy(sample).to(torch.float)\n",
    "    J1_list.append(J1)\n",
    "    J2_list.append(J2)\n",
    "    J3_list.append(J3)\n",
    "    seq = generate_GPTmeasureoutput(J1,J2,J3,model,10,200000,device)[:,3:].to(torch.long)\n",
    "    twopoint = MoM_twopoint(seq,2,device,10)\n",
    "    twopoint_list.append(twopoint)\n",
    "    torch.cuda.empty_cache()\n",
    "# Convert lists to tensors\n",
    "J1_list = torch.tensor(J1_list)\n",
    "J2_list = torch.tensor(J2_list)\n",
    "J3_list = torch.tensor(J3_list)\n",
    "twopoint_list = torch.tensor(twopoint_list)\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=J1_list,\n",
    "    y=J2_list,\n",
    "    z=J3_list,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=twopoint_list,\n",
    "        colorscale='Viridis',  # Color scale\n",
    "        colorbar=dict(title='Energy')\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Update layout for a better look\n",
    "fig.update_layout(\n",
    "    title='Energy Spectrum for Symmetric J1, J2, J3 Grid',\n",
    "    scene=dict(\n",
    "        xaxis_title='J1',\n",
    "        yaxis_title='J2',\n",
    "        zaxis_title='J3'\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
