{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090433fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code of model and prediction function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerBlock_prenorm(nn.Module):\n",
    "    def __init__(self, embed_size, head_count):\n",
    "        super(TransformerBlock_modify_pre, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, head_count, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(4*embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attention_output, _ = self.attention(x_norm,x_norm,x_norm)\n",
    "        x = x + self.dropout(attention_output)\n",
    "        x_norm = self.norm2(x)\n",
    "        ff_output = self.feed_forward(x_norm)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, posi_size, embed_size, num_layers, head_count):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.posi_size = posi_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(posi_size, embed_size)\n",
    "        self.para_embedding = nn.Sequential(\n",
    "            nn.Linear(1, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size,embed_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(embed_size*2),\n",
    "            nn.Linear(embed_size*2, embed_size)\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock_prenorm(embed_size, head_count) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, 2)\n",
    "    def forward(self, inputs, device, mask=None):\n",
    "        inputs = inputs.to(device)\n",
    "        h = inputs[:, 0].view(-1, 1).float()\n",
    "        input_tokens = inputs[:, 1:].to(torch.long)\n",
    "        batch_size, token_count = input_tokens.shape[:2]\n",
    "        # parameter embedding (as prompt)\n",
    "        h_embedding = self.para_embedding(h).unsqueeze(1)  # (batch_size, 1, embed_size)       \n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.word_embedding(input_tokens)\n",
    "        positions = torch.arange(0, token_count).expand(batch_size, token_count).to(device)\n",
    "        position_embeddings = self.position_embedding(positions)\n",
    "        \n",
    "        out = token_embeddings+position_embeddings      \n",
    "        # Concatenate parameter embedding with token embeddings\n",
    "        out = torch.cat((h_embedding, out), dim=1)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)       \n",
    "        out = self.fc_out(out[:, -1, :].reshape(batch_size, self.embed_size)).reshape(batch_size, 2)\n",
    "        return out\n",
    "\n",
    "# predict   \n",
    "def generate_GPTmeasureoutput(h, model, num_qubits, sample_size, device):\n",
    "    outputs = torch.full((sample_size, 1 + num_qubits * 2), h, device=device)  # [h, ..., ...]   \n",
    "    with torch.no_grad():\n",
    "        # Iterate over the qubits\n",
    "        for i in range(num_qubits):\n",
    "            # Generate the random measurement input for the current step\n",
    "            random_input = torch.randint(2, 5, (sample_size, 1), device=device)       \n",
    "            outputs[:, 1 + 2 * i] = random_input.squeeze()  # Fill the random input in appropriate columns       \n",
    "            # Forward pass through the model\n",
    "            measurepro = model(outputs[:, :(2 * i + 2)], device)  # Only pass relevant portion of `outputs`\n",
    "            measurepro = torch.nn.functional.softmax(measurepro, dim=1)          \n",
    "            # Generate new index based on softmax probabilities\n",
    "            newindex = torch.bernoulli(measurepro[:, 1].clamp(min=0, max=1)).to(torch.long)      \n",
    "            # Update the outputs tensor with the new index\n",
    "            outputs[:, 2 + 2 * i] = newindex.squeeze()  # Fill in the next position   \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7adeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (for training)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self,measureResults):\n",
    "        super().__init__()\n",
    "        self.results = measureResults\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        squence = self.results[index]\n",
    "        return squence\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.results)\n",
    "    \n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "sampleResults = torch.load('Ising_sym_negative_dataset_h0_0.25_0.4_0.5_0.6_0.75_0.9_1_every10000_10bit_pbc.pt').to(device)\n",
    "train_index = torch.load('train_index_80000.pt') # random indcies generated before\n",
    "test_index = torch.load('test_index_80000.pt')\n",
    "train_dataset = myDataset(measureResults=sampleResults[train_index])\n",
    "test_dataset = myDataset(measureResults=sampleResults[test_index])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(len(train_dataset),len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fa1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train_recursive(model, dataloader, optimizer, loss_fn, epoch, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for batch_idx, dataset in enumerate(dataloader):\n",
    "        dataset = dataset.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batch_size, token_count = dataset.shape[0], dataset.shape[1]\n",
    "        loss = 0\n",
    "\n",
    "        for cur_count in range(2, token_count):\n",
    "            # Identify if the current token is a measurement result\n",
    "            if cur_count % 2 == 0:\n",
    "                # Prepare target vector for CrossEntropyLoss\n",
    "                expected_next_token_indexes = dataset[:, cur_count]\n",
    "                model_input = dataset[:, :cur_count] \n",
    "                outputs = model(model_input, device)\n",
    "                loss += loss_fn(outputs, expected_next_token_indexes.to(torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1} Train Loss: {average_loss:.6f}\")\n",
    "    writer.add_scalar(\"Loss/train\", average_loss, epoch + 1)\n",
    "\n",
    "def test_recursive(model, dataloader, loss_fn, epoch, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dataset in dataloader:\n",
    "            dataset = dataset.to(device)\n",
    "            batch_size, token_count = dataset.shape[0], dataset.shape[1]\n",
    "            loss = 0\n",
    "            for cur_count in range(2, token_count):\n",
    "                if cur_count % 2 == 0:\n",
    "                    expected_next_token_indexes = dataset[:, cur_count]\n",
    "                    model_input = dataset[:, :cur_count]\n",
    "                    outputs = model(model_input, device)\n",
    "                    loss += loss_fn(outputs, expected_next_token_indexes.to(torch.long))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1} Test Loss: {average_loss:.6f}\")\n",
    "    writer.add_scalar(\"Loss/test\", average_loss, epoch + 1)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "vocab_size = 5\n",
    "posi_size = 20\n",
    "embed_size = 128\n",
    "num_layers = 3\n",
    "head_count = 4\n",
    "model = Transformer(vocab_size, posi_size, embed_size, num_layers, head_count)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2, eta_min=0.000001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 63 # 1+2+4+8+16+32\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_recursive(model, train_loader, optimizer, loss_fn, t, device)\n",
    "    scheduler.step()\n",
    "    cur_test_loss = test_recursive(model, test_loader, loss_fn, t, device)\n",
    "    if cur_test_loss < best_test_loss:\n",
    "        torch.save({\n",
    "            'epoch': t,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_fn\n",
    "        }, 'GPT_sym_TFI_prompt_h0_0.25_0.4_0.5_0.6_0.75_0.9_1_10000_10bit_em128_n3_hc4_coscy6_prenorm_jump_best.pth')\n",
    "        best_test_loss = cur_test_loss\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "torch.save({\n",
    "    'epoch': t,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss_fn\n",
    "}, 'GPT_sym_TFI_prompt_h0_0.25_0.4_0.5_0.6_0.75_0.9_1_10000_10bit_em128_n3_hc4_coscy6_prenorm_jump.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eebf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are functions to calculate physical properties (both for true values and predicted values using shadow tomography)\n",
    "import numpy as np\n",
    "X = torch.tensor([[0.,1.],[1.,0.]], dtype = torch.cfloat)\n",
    "Z = torch.tensor([[1.,0.],[0.,-1.]], dtype = torch.cfloat)\n",
    "I = torch.tensor([[1.,0.],[0.,1.]], dtype = torch.cfloat)\n",
    "operator_map = {0:I, 1:X, 2:Z}\n",
    "\n",
    "def correlation_func(sqe_op):\n",
    "    func = operator_map[int(sqe_op[0])]\n",
    "    for i in range(len(sqe_op)-1):\n",
    "        func = torch.kron(func,operator_map[int(sqe_op[i+1])])\n",
    "    return func\n",
    "\n",
    "# periodic boundary condition\n",
    "def Hamiltonian_sym_circle(num_qubits, h):\n",
    "    ham = 0.+0.j\n",
    "    for i in range(num_qubits-1):\n",
    "        Z_index = np.zeros(num_qubits)\n",
    "        X_index = np.zeros(num_qubits)\n",
    "        Z_index[i] = 2\n",
    "        Z_index[i+1] = 2\n",
    "        X_index[i] = 1\n",
    "        ham+= -((1-h)*correlation_func(Z_index)+h*correlation_func(X_index))\n",
    "    X_index = np.zeros(num_qubits)\n",
    "    X_index[-1] = 1\n",
    "    ham += -h*correlation_func(X_index)\n",
    "    Z_index = np.zeros(num_qubits)\n",
    "    Z_index[0], Z_index[-1] = 2, 2\n",
    "    ham -= (1-h)*correlation_func(Z_index)\n",
    "    return ham\n",
    "\n",
    "def symmetry_proj(rho, num_qubits):\n",
    "    op1_index = np.zeros(num_qubits)\n",
    "    op1 = correlation_func(op1_index)\n",
    "    op2_index = np.ones(num_qubits)\n",
    "    op2 = correlation_func(op2_index)\n",
    "    rho_ = rho+op2@rho@op2.H\n",
    "    return rho_/np.trace(rho_)\n",
    "\n",
    "def obtain_eigenrho(num_bits,h):\n",
    "    Ham = Hamiltonian_sym_circle(num_bits,h)\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(Ham)\n",
    "    ground = eigenvectors[:,0].view(-1,1)\n",
    "    rho = ground@ground.H\n",
    "    rho = symmetry_proj(rho,num_bits)\n",
    "    return rho\n",
    "\n",
    "# ground truth\n",
    "# Two point correlation function <Z_i Z_i+d>\n",
    "def ground_cal_averge_two_point_pbc(num_bits, h, d):\n",
    "    rho = obtain_eigenrho(num_bits,h)\n",
    "    nmz = 0\n",
    "    for i in range(num_bits):\n",
    "        ZZ_index = np.zeros(num_bits)\n",
    "        ZZ_index[i] = 2\n",
    "        ZZ_index[(i+d)%num_bits] = 2\n",
    "        ope = correlation_func(ZZ_index)\n",
    "        nmz += torch.trace(rho@ope)\n",
    "    return nmz/num_bits\n",
    "# <X>\n",
    "def ground_cal_magnezition_xpbc(num_bits,h):\n",
    "    rho = obtain_eigenrho(num_bits,h)\n",
    "    mz = 0\n",
    "    for i in range(num_bits):\n",
    "        Z_index = np.zeros(num_bits)\n",
    "        Z_index[i] = 1\n",
    "        ope = correlation_func(Z_index)\n",
    "        mz += torch.trace(rho@ope)\n",
    "    return mz/num_bits\n",
    "\n",
    "# <XX...X> \n",
    "def ground_cal_averge_Xstring_pbc(num_bits, h, d):\n",
    "    rho = obtain_eigenrho(num_bits,h)\n",
    "    nmx = 0\n",
    "    for i in range(num_bits):\n",
    "        Xs_index = np.zeros(num_bits)\n",
    "        for j in range(i, i + d + 1):\n",
    "            Xs_index[j % num_bits] = 1\n",
    "        ope = correlation_func(Xs_index)\n",
    "        nmx += torch.trace(rho@ope)\n",
    "    return nmx/num_bits\n",
    "\n",
    "# GPT\n",
    "# calculate <X> given sequences generated by transformer\n",
    "def cal_magnezition_x(sqe):\n",
    "    num_bits = int(sqe.shape[1]/2)\n",
    "    mz = 0\n",
    "    for i in range(num_bits):\n",
    "        cond_Z_p = (sqe[:,i*2]==2) & (sqe[:,i*2+1]==1)\n",
    "        cond_Z_n = (sqe[:,i*2]==2) & (sqe[:,i*2+1]==0)\n",
    "        tempz = torch.sum(cond_Z_p)*3-torch.sum(cond_Z_n)*3\n",
    "        mz += tempz\n",
    "    return mz/(num_bits*sqe.shape[0])\n",
    "\n",
    "# calculate E(<Z_i Z_i+d>) given sequences generated by transformer\n",
    "def cal_averge_two_point_pbc(sqe, d):\n",
    "    num_bits = int(sqe.shape[1]/2)\n",
    "    nmz = 0\n",
    "    expZ = torch.zeros(sqe.shape[0], sqe.shape[1] // 2, dtype=sqe.dtype)\n",
    "    cond_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 1)\n",
    "    cond_neg_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 0)\n",
    "    for i in range(0, expZ.shape[1] * 2, 2):\n",
    "        expZ[:, i // 2][cond_3_Z[:, i]] = 3\n",
    "        expZ[:, i // 2][cond_neg_3_Z[:, i]] = -3\n",
    "    expZZ = expZ* torch.cat((expZ[:,d:],expZ[:,:d]),dim=1)\n",
    "    return torch.sum(expZZ)/(num_bits*sqe.shape[0])\n",
    "\n",
    "# calculate E(<X...X>), here d = l-1, d=0 means calculating <X> given sequences generated by transformer\n",
    "def cal_averge_Xstring_pbc(sqe, d):\n",
    "    if d == 0:\n",
    "        return cal_magnezition_x(sqe)\n",
    "    else:\n",
    "        num_bits = int(sqe.shape[1] / 2)\n",
    "        nmx = 0\n",
    "        expX = torch.zeros(sqe.shape[0], sqe.shape[1] // 2, dtype=sqe.dtype)  \n",
    "        cond_3_X = (sqe[:, :-1] == 2) & (sqe[:, 1:] == 1)\n",
    "        cond_neg_3_X = (sqe[:, :-1] == 2) & (sqe[:, 1:] == 0)    \n",
    "        for i in range(0, expX.shape[1] * 2, 2):\n",
    "            expX[:, i // 2][cond_3_X[:, i]] = 3\n",
    "            expX[:, i // 2][cond_neg_3_X[:, i]] = -3   \n",
    "        expXs = expX.clone()\n",
    "        for shift in range(1, d + 1):\n",
    "            expXs *= torch.cat((expX[:, shift:], expX[:, :shift]), dim=1)\n",
    "        return torch.sum(expXs) / (num_bits * sqe.shape[0])\n",
    "    \n",
    "# calculate ground energy given sequences generated by transformer\n",
    "def cal_ground_pbc(h,sqe):\n",
    "    expX = torch.zeros(sqe.shape[0], sqe.shape[1] // 2, dtype=sqe.dtype)\n",
    "    cond_3_X = (sqe[:, :-1] == 2) & (sqe[:, 1:] == 1)\n",
    "    cond_neg_3_X = (sqe[:, :-1] == 2) & (sqe[:, 1:] == 0)\n",
    "    for i in range(0, expX.shape[1] * 2, 2):\n",
    "        expX[:, i // 2][cond_3_X[:, i]] = 3\n",
    "        expX[:, i // 2][cond_neg_3_X[:, i]] = -3\n",
    "        \n",
    "    expZ = torch.zeros(sqe.shape[0], sqe.shape[1] // 2, dtype=sqe.dtype)\n",
    "    cond_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 1)\n",
    "    cond_neg_3_Z = (sqe[:, :-1] == 4) & (sqe[:, 1:] == 0)\n",
    "    for i in range(0, expZ.shape[1] * 2, 2):\n",
    "        expZ[:, i // 2][cond_3_Z[:, i]] = 3\n",
    "        expZ[:, i // 2][cond_neg_3_Z[:, i]] = -3\n",
    "    expZZ = expZ[:,:-1]*expZ[:,1:]\n",
    "\n",
    "    T = sqe.shape[0]\n",
    "    return -((1-h)*torch.sum(expZZ) + (1-h)*torch.sum(expZ[:,-1]*expZ[:,0]) + h*torch.sum(expX))/T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0947f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to implement median of means\n",
    "\n",
    "def median_of_means(h, sqe, num_parts):\n",
    "    T, _ = sqe.shape\n",
    "    part_size = T // num_parts\n",
    "    \n",
    "    means = []\n",
    "    for i in range(num_parts):\n",
    "        part_sqe = sqe[i*part_size : (i+1)*part_size]\n",
    "        mean_value = cal_ground_pbc(h, part_sqe)\n",
    "        means.append(mean_value.item())\n",
    "    \n",
    "    median_value = torch.median(torch.tensor(means))\n",
    "    return median_value\n",
    "def median_of_means_two(d, sqe, num_parts):\n",
    "    T, _ = sqe.shape\n",
    "    part_size = T // num_parts\n",
    "    \n",
    "    means = []\n",
    "    for i in range(num_parts):\n",
    "        part_sqe = sqe[i*part_size : (i+1)*part_size]\n",
    "        mean_value = cal_averge_two_point_pbc(part_sqe,d)\n",
    "        means.append(mean_value.item())\n",
    "    \n",
    "    median_value = torch.median(torch.tensor(means))\n",
    "    return median_value\n",
    "def median_of_means_X(d, sqe, num_parts):\n",
    "    T, _ = sqe.shape\n",
    "    part_size = T // num_parts\n",
    "    \n",
    "    means = []\n",
    "    for i in range(num_parts):\n",
    "        part_sqe = sqe[i*part_size : (i+1)*part_size]\n",
    "        mean_value = cal_averge_Xstring_pbc(part_sqe,d)\n",
    "        means.append(mean_value.item())\n",
    "    \n",
    "    median_value = torch.median(torch.tensor(means))\n",
    "    return median_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf32be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "\n",
    "import torch\n",
    "\n",
    "vocab_size = 5\n",
    "posi_size = 20\n",
    "embed_size = 128\n",
    "num_layers = 3\n",
    "head_count = 4\n",
    "model = Transformer_prompt(vocab_size, posi_size, embed_size, num_layers, head_count)\n",
    "checkpoint = torch.load('GPT_sym_TFI_prompt_h0_0.25_0.4_0.5_0.6_0.75_0.9_1_10000_10bit_em128_n3_hc4_coscy6_prenorm_jump_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "gpt_eval_points = torch.zeros((6, 5, 41))\n",
    "gpt_eval_pointsX = torch.zeros((6, 5, 41))\n",
    "gpt_eval_points_energy = torch.zeros((6, 41))\n",
    "true_ZZ = torch.zeros(5, 101)\n",
    "true_stringX = torch.zeros(5, 101)\n",
    "\n",
    "hs = torch.linspace(0, 1, 41)\n",
    "\n",
    "# Loop to generate sequences and calculate values for all metrics\n",
    "for m in range(6):\n",
    "    for j in range(41):\n",
    "        h = hs[j]\n",
    "        sqe = generate_GPTmeasureoutput(h, model, 10, 300000, device)[:, 1:].to(torch.long)\n",
    "\n",
    "        # Calculate different metrics\n",
    "        for i in range(5):\n",
    "            gpt_eval_points[m, i, j] = median_of_means_two(i + 1, sqe, 10) # two point correlation function\n",
    "            gpt_eval_pointsX[m, i, j] = median_of_means_X(i, sqe, 10) # Xstring\n",
    "\n",
    "        gpt_eval_points_energy[m, j] = median_of_means(h, sqe, 10) # ground energy\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Save results\n",
    "torch.save(gpt_eval_points, 'Newarc_sym_6times_TFI_two_ZZ_d_gpt_es128_30w_MoM10.pt')\n",
    "torch.save(gpt_eval_pointsX, 'Newarc_sym_6times_TFI_Xs_d_gpt_es128_30w_MoM10.pt')\n",
    "torch.save(gpt_eval_points_energy, 'Newarc_sym_TFI_Energy_gpt_es128_30w_MoM10.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
